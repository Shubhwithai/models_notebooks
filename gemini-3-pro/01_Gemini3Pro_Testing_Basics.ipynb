{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
    "\n",
    "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
    "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/[NOTEBOOK_ID])\n",
    "\n",
    "## Master Generative AI in 8 Weeks\n",
    "**What You'll Learn:**\n",
    "- Master cutting-edge AI tools & frameworks\n",
    "- 6 weeks of hands-on, project-based learning\n",
    "- Weekly live mentorship sessions\n",
    "- No coding experience required\n",
    "- Join Innovation Community\n",
    "\n",
    "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
    "\n",
    "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini 3 Pro - Testing & Basics\n",
    "\n",
    "**Created by:** @BuildFastWithAI  \n",
    "**Model:** Google Gemini 3 Pro  \n",
    "**Last Updated:** November 2025\n",
    "\n",
    "This notebook covers the fundamentals of working with Gemini 3 Pro, including:\n",
    "- Basic setup and hello world examples\n",
    "- Tool calling capabilities\n",
    "- Simple agent implementation\n",
    "- Quick RAG demo\n",
    "- Performance metrics and cost estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-generativeai langchain langchain-google-genai faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.colab import userdata\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Configure API key from Colab secrets\n",
    "try:\n",
    "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "    print(\"âœ… API key configured successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(\"Please add GOOGLE_API_KEY to Colab secrets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Example - Hello World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = genai.GenerativeModel('gemini-3-pro')\n",
    "\n",
    "# Simple generation\n",
    "response = model.generate_content(\"Hello! Introduce yourself and explain what makes you unique.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with parameters\n",
    "generation_config = {\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"max_output_tokens\": 1024,\n",
    "}\n",
    "\n",
    "model_configured = genai.GenerativeModel(\n",
    "    'gemini-3-pro',\n",
    "    generation_config=generation_config\n",
    ")\n",
    "\n",
    "response = model_configured.generate_content(\n",
    "    \"Write a creative short story about AI in 3 sentences.\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools\n",
    "def get_weather(location: str) -> dict:\n",
    "    \"\"\"Get weather information for a location.\"\"\"\n",
    "    # Simulated response\n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"temperature\": 72,\n",
    "        \"condition\": \"Sunny\",\n",
    "        \"humidity\": 45\n",
    "    }\n",
    "\n",
    "def calculator(operation: str, num1: float, num2: float) -> float:\n",
    "    \"\"\"Perform basic arithmetic operations.\"\"\"\n",
    "    operations = {\n",
    "        \"add\": num1 + num2,\n",
    "        \"subtract\": num1 - num2,\n",
    "        \"multiply\": num1 * num2,\n",
    "        \"divide\": num1 / num2 if num2 != 0 else \"Error: Division by zero\"\n",
    "    }\n",
    "    return operations.get(operation, \"Invalid operation\")\n",
    "\n",
    "# Define function declarations for Gemini\n",
    "tools = [\n",
    "    {\n",
    "        \"function_declarations\": [\n",
    "            {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"description\": \"Get weather information for a specific location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"City name or location\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"location\"]\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"calculator\",\n",
    "                \"description\": \"Perform arithmetic operations\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"operation\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"]\n",
    "                        },\n",
    "                        \"num1\": {\"type\": \"number\"},\n",
    "                        \"num2\": {\"type\": \"number\"}\n",
    "                    },\n",
    "                    \"required\": [\"operation\", \"num1\", \"num2\"]\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "model_with_tools = genai.GenerativeModel('gemini-3-pro', tools=tools)\n",
    "chat = model_with_tools.start_chat()\n",
    "\n",
    "response = chat.send_message(\"What's the weather in San Francisco and calculate 25 * 4?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simple Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple ReAct agent\n",
    "class SimpleAgent:\n",
    "    def __init__(self, model_name='gemini-3-pro'):\n",
    "        self.model = genai.GenerativeModel(model_name, tools=tools)\n",
    "        self.chat = self.model.start_chat()\n",
    "        self.tool_map = {\n",
    "            \"get_weather\": get_weather,\n",
    "            \"calculator\": calculator\n",
    "        }\n",
    "    \n",
    "    def run(self, query: str, max_iterations: int = 5):\n",
    "        print(f\"Query: {query}\\n\")\n",
    "        \n",
    "        for i in range(max_iterations):\n",
    "            response = self.chat.send_message(query)\n",
    "            \n",
    "            # Check if model wants to call a function\n",
    "            if hasattr(response, 'parts'):\n",
    "                for part in response.parts:\n",
    "                    if hasattr(part, 'function_call'):\n",
    "                        func_call = part.function_call\n",
    "                        func_name = func_call.name\n",
    "                        func_args = dict(func_call.args)\n",
    "                        \n",
    "                        print(f\"ðŸ”§ Calling tool: {func_name}\")\n",
    "                        print(f\"   Args: {func_args}\")\n",
    "                        \n",
    "                        # Execute function\n",
    "                        result = self.tool_map[func_name](**func_args)\n",
    "                        print(f\"   Result: {result}\\n\")\n",
    "                        \n",
    "                        # Send result back to model\n",
    "                        query = str(result)\n",
    "                        continue\n",
    "            \n",
    "            # If no function call, we have final answer\n",
    "            print(f\"Final Answer: {response.text}\")\n",
    "            break\n",
    "\n",
    "# Test agent\n",
    "agent = SimpleAgent()\n",
    "agent.run(\"What's the weather in New York and what's 150 divided by 5?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quick RAG Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Gemini 3 Pro is Google's latest large language model with enhanced reasoning capabilities.\",\n",
    "    \"The model supports multimodal inputs including text, images, and audio.\",\n",
    "    \"Gemini 3 Pro offers improved context understanding and can handle up to 1M tokens.\",\n",
    "    \"It features advanced function calling and structured output generation.\",\n",
    "    \"The model is optimized for low latency and cost-effective inference.\"\n",
    "]\n",
    "\n",
    "# Create embeddings and vector store\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "texts = text_splitter.create_documents(documents)\n",
    "\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# Create QA chain\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-3-pro\", temperature=0.3)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    ")\n",
    "\n",
    "# Query\n",
    "query = \"What are the key features of Gemini 3 Pro?\"\n",
    "response = qa_chain.run(query)\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"\\nAnswer: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer Support Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_support_prompt = \"\"\"\n",
    "You are a helpful customer support agent. Assist users with their queries professionally.\n",
    "User Query: {query}\n",
    "\"\"\"\n",
    "\n",
    "support_model = genai.GenerativeModel('gemini-3-pro')\n",
    "query = \"I can't log into my account. What should I do?\"\n",
    "response = support_model.generate_content(customer_support_prompt.format(query=query))\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_prompt = \"Write a Python function to find the nth Fibonacci number with memoization.\"\n",
    "response = model.generate_content(code_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"sales\": [1000, 1500, 1200, 1800, 2000],\n",
    "    \"months\": [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\"]\n",
    "}\n",
    "\n",
    "analysis_prompt = f\"\"\"\n",
    "Analyze this sales data and provide insights:\n",
    "{json.dumps(data, indent=2)}\n",
    "\n",
    "Include:\n",
    "1. Trends\n",
    "2. Growth rate\n",
    "3. Recommendations\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(analysis_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Metrics & Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Benchmark\n",
    "test_prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"Write a haiku about AI.\"\n",
    "]\n",
    "\n",
    "total_time = 0\n",
    "total_tokens = 0\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    start = time.time()\n",
    "    response = model.generate_content(prompt)\n",
    "    elapsed = time.time() - start\n",
    "    total_time += elapsed\n",
    "    \n",
    "    # Estimate tokens (rough calculation)\n",
    "    tokens = len(prompt.split()) + len(response.text.split())\n",
    "    total_tokens += tokens\n",
    "    \n",
    "    print(f\"Prompt: {prompt[:50]}...\")\n",
    "    print(f\"Time: {elapsed:.2f}s | Est. Tokens: {tokens}\\n\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Summary:\")\n",
    "print(f\"Total Time: {total_time:.2f}s\")\n",
    "print(f\"Average Time: {total_time/len(test_prompts):.2f}s\")\n",
    "print(f\"Total Tokens: ~{total_tokens}\")\n",
    "print(f\"\\nðŸ’° Estimated Cost (assuming $0.001/1K tokens): ${(total_tokens/1000) * 0.001:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "\n",
    "âœ… **Strengths:**\n",
    "- Fast response times and low latency\n",
    "- Excellent tool calling capabilities\n",
    "- Strong reasoning and context understanding\n",
    "- Cost-effective for production use\n",
    "\n",
    "ðŸ“Œ **Best Practices:**\n",
    "- Use Colab secrets for API keys\n",
    "- Implement error handling for production\n",
    "- Monitor token usage and costs\n",
    "- Test with different temperature settings\n",
    "\n",
    "ðŸ”— **Resources:**\n",
    "- [Official Documentation](https://ai.google.dev/)\n",
    "- [API Reference](https://ai.google.dev/api)\n",
    "- Follow [@BuildFastWithAI](https://twitter.com/BuildFastWithAI) for more tutorials"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
