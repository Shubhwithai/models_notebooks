{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
    "\n",
    "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
    "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/[NOTEBOOK_ID])\n",
    "\n",
    "## Master Generative AI in 8 Weeks\n",
    "**What You'll Learn:**\n",
    "- Master cutting-edge AI tools & frameworks\n",
    "- 6 weeks of hands-on, project-based learning\n",
    "- Weekly live mentorship sessions\n",
    "- No coding experience required\n",
    "- Join Innovation Community\n",
    "\n",
    "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
    "\n",
    "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini 3 Pro - Advanced Features\n",
    "\n",
    "**Created by:** @BuildFastWithAI  \n",
    "**Model:** Google Gemini 3 Pro  \n",
    "**Last Updated:** November 2025\n",
    "\n",
    "This notebook covers advanced features including:\n",
    "- Streaming responses\n",
    "- Advanced function calling\n",
    "- Structured output and JSON mode\n",
    "- Context management\n",
    "- Caching and optimization\n",
    "- Error handling and retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-generativeai pydantic tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.colab import userdata\n",
    "import json\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "# Configure API\n",
    "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Streaming Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic streaming\n",
    "model = genai.GenerativeModel('gemini-3-pro')\n",
    "\n",
    "response = model.generate_content(\n",
    "    \"Write a detailed explanation of neural networks.\",\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "print(\"Streaming response:\")\n",
    "for chunk in response:\n",
    "    print(chunk.text, end='', flush=True)\n",
    "print(\"\\n\\n‚úÖ Streaming complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming with function calling\n",
    "tools = [\n",
    "    {\n",
    "        \"function_declarations\": [\n",
    "            {\n",
    "                \"name\": \"search_web\",\n",
    "                \"description\": \"Search the web for information\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"query\": {\"type\": \"string\", \"description\": \"Search query\"}\n",
    "                    },\n",
    "                    \"required\": [\"query\"]\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "model_with_tools = genai.GenerativeModel('gemini-3-pro', tools=tools)\n",
    "chat = model_with_tools.start_chat()\n",
    "\n",
    "response = chat.send_message(\n",
    "    \"Search for the latest AI news and summarize it.\",\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if chunk.text:\n",
    "        print(chunk.text, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel function calls\n",
    "def get_stock_price(symbol: str) -> dict:\n",
    "    \"\"\"Get stock price for a symbol.\"\"\"\n",
    "    # Simulated data\n",
    "    prices = {\"AAPL\": 175.50, \"GOOGL\": 142.30, \"MSFT\": 380.20}\n",
    "    return {\"symbol\": symbol, \"price\": prices.get(symbol, 0)}\n",
    "\n",
    "def get_company_info(symbol: str) -> dict:\n",
    "    \"\"\"Get company information.\"\"\"\n",
    "    info = {\n",
    "        \"AAPL\": {\"name\": \"Apple Inc.\", \"sector\": \"Technology\"},\n",
    "        \"GOOGL\": {\"name\": \"Alphabet Inc.\", \"sector\": \"Technology\"},\n",
    "        \"MSFT\": {\"name\": \"Microsoft Corp.\", \"sector\": \"Technology\"}\n",
    "    }\n",
    "    return info.get(symbol, {})\n",
    "\n",
    "# Function schemas\n",
    "finance_tools = [\n",
    "    {\n",
    "        \"function_declarations\": [\n",
    "            {\n",
    "                \"name\": \"get_stock_price\",\n",
    "                \"description\": \"Get current stock price\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"symbol\": {\"type\": \"string\", \"description\": \"Stock symbol\"}\n",
    "                    },\n",
    "                    \"required\": [\"symbol\"]\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"get_company_info\",\n",
    "                \"description\": \"Get company information\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"symbol\": {\"type\": \"string\", \"description\": \"Stock symbol\"}\n",
    "                    },\n",
    "                    \"required\": [\"symbol\"]\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "model = genai.GenerativeModel('gemini-3-pro', tools=finance_tools)\n",
    "chat = model.start_chat()\n",
    "\n",
    "response = chat.send_message(\n",
    "    \"Get me the stock price and company info for AAPL and GOOGL\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Structured Output with Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define structured output schema\n",
    "class ProductReview(BaseModel):\n",
    "    product_name: str = Field(description=\"Name of the product\")\n",
    "    rating: int = Field(description=\"Rating from 1-5\", ge=1, le=5)\n",
    "    pros: List[str] = Field(description=\"List of pros\")\n",
    "    cons: List[str] = Field(description=\"List of cons\")\n",
    "    summary: str = Field(description=\"Brief summary\")\n",
    "    would_recommend: bool = Field(description=\"Whether to recommend\")\n",
    "\n",
    "# Create prompt for structured output\n",
    "review_text = \"\"\"\n",
    "I bought the XYZ Laptop last month. It has amazing battery life and a beautiful display.\n",
    "The performance is great for coding and video editing. However, it's quite expensive\n",
    "and the trackpad could be better. Overall, I'm happy with my purchase.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Analyze this product review and return ONLY valid JSON matching this schema:\n",
    "{ProductReview.model_json_schema()}\n",
    "\n",
    "Review: {review_text}\n",
    "\n",
    "Return JSON only, no additional text.\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-3-pro',\n",
    "    generation_config={\"response_mime_type\": \"application/json\"}\n",
    ")\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "review_data = json.loads(response.text)\n",
    "print(json.dumps(review_data, indent=2))\n",
    "\n",
    "# Validate with Pydantic\n",
    "validated_review = ProductReview(**review_data)\n",
    "print(f\"\\n‚úÖ Validated: {validated_review.product_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Prompting Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot learning\n",
    "few_shot_prompt = \"\"\"\n",
    "Classify the sentiment of customer feedback.\n",
    "\n",
    "Examples:\n",
    "Input: \"The product is amazing! Best purchase ever.\"\n",
    "Output: {\"sentiment\": \"positive\", \"confidence\": 0.95}\n",
    "\n",
    "Input: \"Terrible quality. Very disappointed.\"\n",
    "Output: {\"sentiment\": \"negative\", \"confidence\": 0.90}\n",
    "\n",
    "Input: \"It's okay, nothing special.\"\n",
    "Output: {\"sentiment\": \"neutral\", \"confidence\": 0.75}\n",
    "\n",
    "Now classify:\n",
    "Input: \"Good value for money, but shipping was slow.\"\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-3-pro',\n",
    "    generation_config={\"response_mime_type\": \"application/json\"}\n",
    ")\n",
    "response = model.generate_content(few_shot_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain-of-thought prompting\n",
    "cot_prompt = \"\"\"\n",
    "Solve this problem step by step:\n",
    "\n",
    "A store has 150 items. On Monday, they sold 30% of their inventory.\n",
    "On Tuesday, they received a shipment of 50 new items.\n",
    "On Wednesday, they sold 25 items.\n",
    "How many items do they have now?\n",
    "\n",
    "Think through this step by step:\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-3-pro')\n",
    "response = model.generate_content(cot_prompt)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Context Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation with context window management\n",
    "class ManagedChat:\n",
    "    def __init__(self, max_history: int = 10):\n",
    "        self.model = genai.GenerativeModel('gemini-3-pro')\n",
    "        self.chat = self.model.start_chat()\n",
    "        self.max_history = max_history\n",
    "        self.messages = []\n",
    "    \n",
    "    def send_message(self, message: str) -> str:\n",
    "        # Add user message\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "        \n",
    "        # Truncate if needed\n",
    "        if len(self.messages) > self.max_history:\n",
    "            self.messages = self.messages[-self.max_history:]\n",
    "        \n",
    "        # Send message\n",
    "        response = self.chat.send_message(message)\n",
    "        \n",
    "        # Add assistant message\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": response.text})\n",
    "        \n",
    "        return response.text\n",
    "    \n",
    "    def get_context_size(self) -> int:\n",
    "        return len(self.messages)\n",
    "\n",
    "# Test managed chat\n",
    "chat = ManagedChat(max_history=6)\n",
    "print(chat.send_message(\"Hello! I'm learning about AI.\"))\n",
    "print(f\"\\nContext size: {chat.get_context_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Processing & Rate Limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "class RateLimitedBatchProcessor:\n",
    "    def __init__(self, requests_per_minute: int = 60):\n",
    "        self.model = genai.GenerativeModel('gemini-3-pro')\n",
    "        self.delay = 60.0 / requests_per_minute\n",
    "        self.last_request = 0\n",
    "    \n",
    "    def process_with_rate_limit(self, prompt: str) -> str:\n",
    "        # Rate limiting\n",
    "        now = time.time()\n",
    "        time_since_last = now - self.last_request\n",
    "        if time_since_last < self.delay:\n",
    "            time.sleep(self.delay - time_since_last)\n",
    "        \n",
    "        self.last_request = time.time()\n",
    "        response = self.model.generate_content(prompt)\n",
    "        return response.text\n",
    "    \n",
    "    def batch_process(self, prompts: List[str], max_workers: int = 3) -> List[str]:\n",
    "        results = []\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_prompt = {\n",
    "                executor.submit(self.process_with_rate_limit, prompt): prompt \n",
    "                for prompt in prompts\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(future_to_prompt):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    results.append(None)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Test batch processing\n",
    "processor = RateLimitedBatchProcessor(requests_per_minute=30)\n",
    "prompts = [\n",
    "    \"Explain quantum computing in one sentence.\",\n",
    "    \"What is machine learning?\",\n",
    "    \"Define artificial intelligence.\"\n",
    "]\n",
    "\n",
    "print(\"Processing batch...\")\n",
    "results = processor.batch_process(prompts, max_workers=2)\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i}: {result[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Caching & Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import hashlib\n",
    "\n",
    "class CachedModel:\n",
    "    def __init__(self):\n",
    "        self.model = genai.GenerativeModel('gemini-3-pro')\n",
    "        self.cache = {}\n",
    "    \n",
    "    def _hash_prompt(self, prompt: str) -> str:\n",
    "        return hashlib.md5(prompt.encode()).hexdigest()\n",
    "    \n",
    "    def generate(self, prompt: str, use_cache: bool = True) -> str:\n",
    "        if use_cache:\n",
    "            cache_key = self._hash_prompt(prompt)\n",
    "            if cache_key in self.cache:\n",
    "                print(\"‚úÖ Cache hit!\")\n",
    "                return self.cache[cache_key]\n",
    "        \n",
    "        print(\"üîÑ Generating new response...\")\n",
    "        response = self.model.generate_content(prompt)\n",
    "        \n",
    "        if use_cache:\n",
    "            self.cache[cache_key] = response.text\n",
    "        \n",
    "        return response.text\n",
    "\n",
    "# Test caching\n",
    "cached_model = CachedModel()\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "print(\"First call:\")\n",
    "result1 = cached_model.generate(prompt)\n",
    "print(result1[:100])\n",
    "\n",
    "print(\"\\nSecond call (should use cache):\")\n",
    "result2 = cached_model.generate(prompt)\n",
    "print(result2[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Handling & Retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "from google.api_core import exceptions\n",
    "\n",
    "class RobustModel:\n",
    "    def __init__(self):\n",
    "        self.model = genai.GenerativeModel('gemini-3-pro')\n",
    "    \n",
    "    @retry(\n",
    "        stop=stop_after_attempt(3),\n",
    "        wait=wait_exponential(multiplier=1, min=2, max=10),\n",
    "        retry=retry_if_exception_type((exceptions.ResourceExhausted, exceptions.ServiceUnavailable))\n",
    "    )\n",
    "    def generate_with_retry(self, prompt: str) -> str:\n",
    "        \"\"\"Generate content with automatic retry on transient errors.\"\"\"\n",
    "        try:\n",
    "            response = self.model.generate_content(prompt)\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error occurred: {type(e).__name__}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_safe(self, prompt: str, default: str = \"Error occurred\") -> str:\n",
    "        \"\"\"Generate with fallback on error.\"\"\"\n",
    "        try:\n",
    "            return self.generate_with_retry(prompt)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå All retries failed: {e}\")\n",
    "            return default\n",
    "\n",
    "# Test error handling\n",
    "robust_model = RobustModel()\n",
    "result = robust_model.generate_safe(\"Explain machine learning.\")\n",
    "print(result[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "‚úÖ **Advanced Capabilities:**\n",
    "- Streaming for real-time responses\n",
    "- Parallel function calling\n",
    "- Structured JSON output with validation\n",
    "- Sophisticated prompting techniques\n",
    "\n",
    "üîß **Production Best Practices:**\n",
    "- Implement rate limiting\n",
    "- Use caching for repeated queries\n",
    "- Add retry logic with exponential backoff\n",
    "- Manage context window efficiently\n",
    "\n",
    "üìä **Optimization Tips:**\n",
    "- Batch similar requests\n",
    "- Use streaming for long outputs\n",
    "- Cache common responses\n",
    "- Monitor token usage\n",
    "\n",
    "üîó **Resources:**\n",
    "- [Advanced Features Docs](https://ai.google.dev/)\n",
    "- Follow [@BuildFastWithAI](https://twitter.com/BuildFastWithAI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
