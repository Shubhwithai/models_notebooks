{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
    "\n",
    "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
    "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/[NOTEBOOK_ID])\n",
    "\n",
    "## Master Generative AI in 8 Weeks\n",
    "**What You'll Learn:**\n",
    "- Master cutting-edge AI tools & frameworks\n",
    "- 6 weeks of hands-on, project-based learning\n",
    "- Weekly live mentorship sessions\n",
    "- No coding experience required\n",
    "- Join Innovation Community\n",
    "\n",
    "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
    "\n",
    "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini 3 Pro - Advanced RAG Techniques\n",
    "\n",
    "**Created by:** @BuildFastWithAI  \n",
    "**Model:** Google Gemini 3 Pro  \n",
    "**Last Updated:** November 2025\n",
    "\n",
    "Advanced RAG implementations with hybrid search, reranking, and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-generativeai langchain langchain-google-genai faiss-cpu rank-bm25 sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.colab import userdata\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hybrid Search (BM25 + Vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRetriever:\n",
    "    def __init__(self, embeddings, alpha=0.5):\n",
    "        self.embeddings = embeddings\n",
    "        self.alpha = alpha  # Weight for vector search\n",
    "        self.vectorstore = None\n",
    "        self.bm25 = None\n",
    "        self.documents = []\n",
    "    \n",
    "    def add_documents(self, docs):\n",
    "        self.documents = docs\n",
    "        \n",
    "        # Vector store\n",
    "        self.vectorstore = FAISS.from_documents(docs, self.embeddings)\n",
    "        \n",
    "        # BM25 index\n",
    "        tokenized_docs = [doc.page_content.split() for doc in docs]\n",
    "        self.bm25 = BM25Okapi(tokenized_docs)\n",
    "    \n",
    "    def retrieve(self, query, k=5):\n",
    "        # Vector search\n",
    "        vector_docs = self.vectorstore.similarity_search_with_score(query, k=k*2)\n",
    "        \n",
    "        # BM25 search\n",
    "        tokenized_query = query.split()\n",
    "        bm25_scores = self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        # Normalize scores\n",
    "        vector_scores = {i: 1/(1+score) for i, (doc, score) in enumerate(vector_docs)}\n",
    "        bm25_scores_norm = bm25_scores / (bm25_scores.max() + 1e-6)\n",
    "        \n",
    "        # Combine scores\n",
    "        combined_scores = {}\n",
    "        for i in range(len(self.documents)):\n",
    "            vec_score = vector_scores.get(i, 0)\n",
    "            bm_score = bm25_scores_norm[i]\n",
    "            combined_scores[i] = self.alpha * vec_score + (1 - self.alpha) * bm_score\n",
    "        \n",
    "        # Get top k\n",
    "        top_indices = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "        return [self.documents[i] for i, _ in top_indices]\n",
    "\n",
    "# Test hybrid search\n",
    "from langchain.schema import Document\n",
    "\n",
    "docs = [\n",
    "    Document(page_content=\"Gemini 3 Pro has a 1 million token context window.\"),\n",
    "    Document(page_content=\"The model excels at mathematical reasoning and code generation.\"),\n",
    "    Document(page_content=\"Multimodal capabilities include text, image, and audio processing.\")\n",
    "]\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "hybrid = HybridRetriever(embeddings)\n",
    "hybrid.add_documents(docs)\n",
    "\n",
    "results = hybrid.retrieve(\"context window size\", k=2)\n",
    "for doc in results:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Query Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-query expansion\n",
    "class QueryExpander:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def expand_query(self, query: str, num_variants: int = 3) -> list:\n",
    "        prompt = f\"\"\"\n",
    "Generate {num_variants} different ways to ask this question:\n",
    "\n",
    "Original: {query}\n",
    "\n",
    "Return only the variants, one per line.\n",
    "\"\"\"\n",
    "        response = self.llm.predict(prompt)\n",
    "        variants = [line.strip() for line in response.split('\\n') if line.strip()]\n",
    "        return [query] + variants[:num_variants]\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-3-pro\", temperature=0.7)\n",
    "expander = QueryExpander(llm)\n",
    "\n",
    "queries = expander.expand_query(\"What are the features of Gemini 3 Pro?\")\n",
    "for i, q in enumerate(queries, 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Chunking Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic chunking\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class SemanticChunker:\n",
    "    def __init__(self, similarity_threshold=0.5):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.threshold = similarity_threshold\n",
    "    \n",
    "    def chunk_text(self, text: str) -> list:\n",
    "        sentences = text.split('. ')\n",
    "        embeddings = self.model.encode(sentences)\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = [sentences[0]]\n",
    "        \n",
    "        for i in range(1, len(sentences)):\n",
    "            similarity = cosine_similarity(\n",
    "                [embeddings[i-1]], \n",
    "                [embeddings[i]]\n",
    "            )[0][0]\n",
    "            \n",
    "            if similarity > self.threshold:\n",
    "                current_chunk.append(sentences[i])\n",
    "            else:\n",
    "                chunks.append('. '.join(current_chunk))\n",
    "                current_chunk = [sentences[i]]\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append('. '.join(current_chunk))\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Test\n",
    "text = \"\"\"Gemini 3 Pro is a powerful model. It has many features. \n",
    "The context window is very large. It can handle 1 million tokens. \n",
    "The model is multimodal. It processes text and images.\"\"\"\n",
    "\n",
    "chunker = SemanticChunker()\n",
    "chunks = chunker.chunk_text(text)\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i}: {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-based reranking\n",
    "class LLMReranker:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def rerank(self, query: str, documents: list, top_k: int = 3) -> list:\n",
    "        # Score each document\n",
    "        scored_docs = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            prompt = f\"\"\"\n",
    "Rate how relevant this document is to the query on a scale of 0-10.\n",
    "Return only the number.\n",
    "\n",
    "Query: {query}\n",
    "Document: {doc.page_content}\n",
    "\n",
    "Relevance score:\n",
    "\"\"\"\n",
    "            try:\n",
    "                score = float(self.llm.predict(prompt).strip())\n",
    "            except:\n",
    "                score = 0\n",
    "            \n",
    "            scored_docs.append((doc, score))\n",
    "        \n",
    "        # Sort by score\n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [doc for doc, _ in scored_docs[:top_k]]\n",
    "\n",
    "# Test reranking\n",
    "reranker = LLMReranker(llm)\n",
    "reranked = reranker.rerank(\"context window\", docs, top_k=2)\n",
    "\n",
    "print(\"Reranked results:\")\n",
    "for doc in reranked:\n",
    "    print(f\"- {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Advanced RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRAG:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/embedding-001\",\n",
    "            google_api_key=api_key\n",
    "        )\n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-3-pro\",\n",
    "            google_api_key=api_key\n",
    "        )\n",
    "        self.retriever = HybridRetriever(self.embeddings)\n",
    "        self.reranker = LLMReranker(self.llm)\n",
    "    \n",
    "    def add_documents(self, texts: list):\n",
    "        docs = [Document(page_content=text) for text in texts]\n",
    "        self.retriever.add_documents(docs)\n",
    "    \n",
    "    def query(self, question: str, k: int = 5) -> dict:\n",
    "        # Retrieve with hybrid search\n",
    "        docs = self.retriever.retrieve(question, k=k*2)\n",
    "        \n",
    "        # Rerank\n",
    "        reranked_docs = self.reranker.rerank(question, docs, top_k=k)\n",
    "        \n",
    "        # Generate answer\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in reranked_docs])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "Answer based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "        \n",
    "        answer = self.llm.predict(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": [doc.page_content for doc in reranked_docs]\n",
    "        }\n",
    "\n",
    "# Test\n",
    "advanced_rag = AdvancedRAG(GOOGLE_API_KEY)\n",
    "advanced_rag.add_documents([\n",
    "    \"Gemini 3 Pro has a 1 million token context window.\",\n",
    "    \"The model supports multimodal inputs.\",\n",
    "    \"It excels at code generation and reasoning.\"\n",
    "])\n",
    "\n",
    "result = advanced_rag.query(\"What is the context window?\", k=2)\n",
    "print(f\"Answer: {result['answer']}\\n\")\n",
    "print(f\"Sources: {len(result['sources'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "âœ… **Advanced Techniques:**\n",
    "- Hybrid search (BM25 + vector)\n",
    "- Query transformation and expansion\n",
    "- Semantic chunking\n",
    "- LLM-based reranking\n",
    "\n",
    "ðŸ“Œ **When to Use:**\n",
    "- Hybrid search for better recall\n",
    "- Reranking for precision\n",
    "- Query expansion for complex questions\n",
    "\n",
    "ðŸ”— **Resources:**\n",
    "- Follow [@BuildFastWithAI](https://twitter.com/BuildFastWithAI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
