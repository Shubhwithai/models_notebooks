{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
    "\n",
    "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
    "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/[NOTEBOOK_ID])\n",
    "\n",
    "## Master Generative AI in 8 Weeks\n",
    "**What You'll Learn:**\n",
    "- Master cutting-edge AI tools & frameworks\n",
    "- 6 weeks of hands-on, project-based learning\n",
    "- Weekly live mentorship sessions\n",
    "- No coding experience required\n",
    "- Join Innovation Community\n",
    "\n",
    "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
    "\n",
    "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini 3 Pro - Simple RAG Implementation\n",
    "\n",
    "**Created by:** @BuildFastWithAI  \n",
    "**Model:** Google Gemini 3 Pro  \n",
    "**Last Updated:** November 2025\n",
    "\n",
    "Complete guide to building a RAG (Retrieval-Augmented Generation) system with Gemini 3 Pro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-generativeai langchain langchain-google-genai faiss-cpu pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.colab import userdata\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "\n",
    "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RAG Fundamentals & Architecture\n",
    "\n",
    "RAG combines:\n",
    "- **Retrieval**: Finding relevant documents from a knowledge base\n",
    "- **Augmentation**: Adding retrieved context to the prompt\n",
    "- **Generation**: Using LLM to generate response with context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "sample_docs = \"\"\"\n",
    "Gemini 3 Pro Overview:\n",
    "Gemini 3 Pro is Google's latest large language model featuring enhanced reasoning capabilities.\n",
    "It supports multimodal inputs including text, images, audio, and video.\n",
    "\n",
    "Key Features:\n",
    "- Context window up to 1 million tokens\n",
    "- Advanced function calling\n",
    "- Native multimodal understanding\n",
    "- Low latency responses\n",
    "- Cost-effective pricing\n",
    "\n",
    "Performance:\n",
    "Gemini 3 Pro excels at complex reasoning tasks, code generation, and data analysis.\n",
    "It outperforms previous models on mathematical reasoning and scientific tasks.\n",
    "\n",
    "Use Cases:\n",
    "- Customer support automation\n",
    "- Content generation\n",
    "- Code assistance\n",
    "- Data analysis and insights\n",
    "- Research and summarization\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open('gemini_docs.txt', 'w') as f:\n",
    "    f.write(sample_docs)\n",
    "\n",
    "print(\"âœ… Documents created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split documents\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Create documents\n",
    "docs = [Document(page_content=sample_docs)]\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "print(f\"\\nFirst chunk:\\n{chunks[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "# Test embedding\n",
    "test_embedding = embeddings.embed_query(\"What is Gemini 3 Pro?\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")\n",
    "print(f\"First 5 values: {test_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vector Store Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS vector store\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Save vector store\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "print(\"âœ… Vector store created and saved\")\n",
    "\n",
    "# Load vector store (for later use)\n",
    "# vectorstore = FAISS.load_local(\"faiss_index\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Retrieval - Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple similarity search\n",
    "query = \"What are the key features of Gemini 3 Pro?\"\n",
    "docs = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search with scores\n",
    "docs_with_scores = vectorstore.similarity_search_with_score(query, k=3)\n",
    "\n",
    "for doc, score in docs_with_scores:\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    print(f\"Content: {doc.page_content[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generation - RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-3-pro\",\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "# Create QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Query\n",
    "query = \"What makes Gemini 3 Pro unique?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"\\nAnswer: {result['result']}\")\n",
    "print(f\"\\nSources used: {len(result['source_documents'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAG:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/embedding-001\",\n",
    "            google_api_key=api_key\n",
    "        )\n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-3-pro\",\n",
    "            google_api_key=api_key,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        self.vectorstore = None\n",
    "    \n",
    "    def load_documents(self, texts: list):\n",
    "        \"\"\"Load and process documents.\"\"\"\n",
    "        docs = [Document(page_content=text) for text in texts]\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=50\n",
    "        )\n",
    "        chunks = text_splitter.split_documents(docs)\n",
    "        \n",
    "        self.vectorstore = FAISS.from_documents(chunks, self.embeddings)\n",
    "        print(f\"âœ… Loaded {len(chunks)} chunks\")\n",
    "    \n",
    "    def query(self, question: str, k: int = 3) -> dict:\n",
    "        \"\"\"Query the RAG system.\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            return {\"error\": \"No documents loaded\"}\n",
    "        \n",
    "        # Retrieve\n",
    "        docs = self.vectorstore.similarity_search(question, k=k)\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        # Generate\n",
    "        prompt = f\"\"\"\n",
    "Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "        \n",
    "        response = self.llm.predict(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": response,\n",
    "            \"sources\": [doc.page_content[:100] for doc in docs]\n",
    "        }\n",
    "\n",
    "# Initialize and test\n",
    "rag = SimpleRAG(GOOGLE_API_KEY)\n",
    "rag.load_documents([sample_docs])\n",
    "\n",
    "# Test queries\n",
    "questions = [\n",
    "    \"What is the context window size?\",\n",
    "    \"What are the main use cases?\",\n",
    "    \"How does it perform on reasoning tasks?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    result = rag.query(q)\n",
    "    print(f\"\\nQ: {result['question']}\")\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "âœ… **RAG Pipeline:**\n",
    "1. Load and chunk documents\n",
    "2. Generate embeddings\n",
    "3. Store in vector database\n",
    "4. Retrieve relevant chunks\n",
    "5. Generate answer with context\n",
    "\n",
    "ðŸ“Œ **Best Practices:**\n",
    "- Choose appropriate chunk size (500-1000 tokens)\n",
    "- Use overlap to maintain context\n",
    "- Retrieve 3-5 most relevant chunks\n",
    "- Persist vector store for reuse\n",
    "\n",
    "ðŸ”— **Resources:**\n",
    "- Follow [@BuildFastWithAI](https://twitter.com/BuildFastWithAI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
