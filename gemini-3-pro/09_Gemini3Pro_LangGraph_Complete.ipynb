{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
    "\n",
    "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
    "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/[NOTEBOOK_ID])\n",
    "\n",
    "## Master Generative AI in 8 Weeks\n",
    "**What You'll Learn:**\n",
    "- Master cutting-edge AI tools & frameworks\n",
    "- 6 weeks of hands-on, project-based learning\n",
    "- Weekly live mentorship sessions\n",
    "- No coding experience required\n",
    "- Join Innovation Community\n",
    "\n",
    "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
    "\n",
    "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini 3 Pro - Complete LangGraph Guide\n",
    "\n",
    "**Created by:** @BuildFastWithAI  \n",
    "**Model:** Google Gemini 3 Pro  \n",
    "**Last Updated:** November 2025\n",
    "\n",
    "Build stateful, multi-agent workflows with LangGraph and Gemini 3 Pro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langgraph langchain langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from typing import TypedDict, Annotated\n",
    "from google.colab import userdata\n",
    "import operator\n",
    "import os\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-3-pro\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state\n",
    "class SimpleState(TypedDict):\n",
    "    messages: Annotated[list, operator.add]\n",
    "    count: int\n",
    "\n",
    "# Define nodes\n",
    "def node_1(state: SimpleState):\n",
    "    print(\"Node 1 executing\")\n",
    "    return {\n",
    "        \"messages\": [\"Node 1 processed\"],\n",
    "        \"count\": state.get(\"count\", 0) + 1\n",
    "    }\n",
    "\n",
    "def node_2(state: SimpleState):\n",
    "    print(\"Node 2 executing\")\n",
    "    return {\n",
    "        \"messages\": [\"Node 2 processed\"],\n",
    "        \"count\": state[\"count\"] + 1\n",
    "    }\n",
    "\n",
    "# Build graph\n",
    "workflow = StateGraph(SimpleState)\n",
    "workflow.add_node(\"node_1\", node_1)\n",
    "workflow.add_node(\"node_2\", node_2)\n",
    "\n",
    "workflow.set_entry_point(\"node_1\")\n",
    "workflow.add_edge(\"node_1\", \"node_2\")\n",
    "workflow.add_edge(\"node_2\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Run\n",
    "result = app.invoke({\"messages\": [], \"count\": 0})\n",
    "print(f\"\\nFinal state: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conditional Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalState(TypedDict):\n",
    "    question: str\n",
    "    answer: str\n",
    "    needs_search: bool\n",
    "\n",
    "def analyze_question(state: ConditionalState):\n",
    "    \"\"\"Determine if question needs search.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    needs_search = \"latest\" in question.lower() or \"current\" in question.lower()\n",
    "    return {\"needs_search\": needs_search}\n",
    "\n",
    "def search(state: ConditionalState):\n",
    "    \"\"\"Simulated search.\"\"\"\n",
    "    return {\"answer\": \"Search results: Latest information found.\"}\n",
    "\n",
    "def direct_answer(state: ConditionalState):\n",
    "    \"\"\"Answer without search.\"\"\"\n",
    "    response = llm.predict(state[\"question\"])\n",
    "    return {\"answer\": response}\n",
    "\n",
    "def route(state: ConditionalState):\n",
    "    \"\"\"Route based on needs_search.\"\"\"\n",
    "    if state.get(\"needs_search\", False):\n",
    "        return \"search\"\n",
    "    return \"direct_answer\"\n",
    "\n",
    "# Build graph\n",
    "workflow = StateGraph(ConditionalState)\n",
    "workflow.add_node(\"analyze\", analyze_question)\n",
    "workflow.add_node(\"search\", search)\n",
    "workflow.add_node(\"direct_answer\", direct_answer)\n",
    "\n",
    "workflow.set_entry_point(\"analyze\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"analyze\",\n",
    "    route,\n",
    "    {\n",
    "        \"search\": \"search\",\n",
    "        \"direct_answer\": \"direct_answer\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"search\", END)\n",
    "workflow.add_edge(\"direct_answer\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "result = app.invoke({\"question\": \"What is AI?\", \"answer\": \"\", \"needs_search\": False})\n",
    "print(f\"Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agent Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Calculate mathematical expressions.\"\"\"\n",
    "    try:\n",
    "        return str(eval(expression))\n",
    "    except:\n",
    "        return \"Error\"\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: list\n",
    "    next_action: str\n",
    "    iterations: int\n",
    "\n",
    "tools = [calculator]\n",
    "\n",
    "def agent_node(state: AgentState):\n",
    "    \"\"\"Agent decides next action.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1] if messages else \"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Current task: {last_message}\n",
    "\n",
    "Available tools: {[t.name for t in tools]}\n",
    "\n",
    "What should I do next? Reply with:\n",
    "TOOL: tool_name | input\n",
    "or\n",
    "DONE: final answer\n",
    "\"\"\"\n",
    "    \n",
    "    response = llm.predict(prompt)\n",
    "    \n",
    "    if \"DONE:\" in response:\n",
    "        return {\n",
    "            \"next_action\": \"done\",\n",
    "            \"messages\": [response.split(\"DONE:\")[1].strip()]\n",
    "        }\n",
    "    elif \"TOOL:\" in response:\n",
    "        return {\n",
    "            \"next_action\": \"tool\",\n",
    "            \"messages\": [response]\n",
    "        }\n",
    "    \n",
    "    return {\"next_action\": \"done\", \"messages\": [response]}\n",
    "\n",
    "def tool_node(state: AgentState):\n",
    "    \"\"\"Execute tool.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if \"TOOL:\" in last_message:\n",
    "        parts = last_message.split(\"TOOL:\")[1].split(\"|\")\n",
    "        tool_name = parts[0].strip()\n",
    "        tool_input = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "        \n",
    "        result = calculator(tool_input)\n",
    "        return {\n",
    "            \"next_action\": \"agent\",\n",
    "            \"messages\": [f\"Tool result: {result}\"],\n",
    "            \"iterations\": state.get(\"iterations\", 0) + 1\n",
    "        }\n",
    "    \n",
    "    return {\"next_action\": \"done\"}\n",
    "\n",
    "def should_continue(state: AgentState):\n",
    "    if state.get(\"iterations\", 0) > 5:\n",
    "        return \"done\"\n",
    "    return state.get(\"next_action\", \"done\")\n",
    "\n",
    "# Build graph\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.add_node(\"tool\", tool_node)\n",
    "workflow.add_node(\"done\", lambda x: x)\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tool\": \"tool\",\n",
    "        \"done\": \"done\",\n",
    "        \"agent\": \"agent\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"tool\", \"agent\")\n",
    "workflow.add_edge(\"done\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test\n",
    "result = app.invoke({\n",
    "    \"messages\": [\"Calculate 25 * 48\"],\n",
    "    \"next_action\": \"\",\n",
    "    \"iterations\": 0\n",
    "})\n",
    "print(f\"Result: {result['messages'][-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Agent Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentState(TypedDict):\n",
    "    task: str\n",
    "    research: str\n",
    "    draft: str\n",
    "    final: str\n",
    "\n",
    "def researcher(state: MultiAgentState):\n",
    "    \"\"\"Research agent.\"\"\"\n",
    "    prompt = f\"Research this topic: {state['task']}\"\n",
    "    research = llm.predict(prompt)\n",
    "    return {\"research\": research}\n",
    "\n",
    "def writer(state: MultiAgentState):\n",
    "    \"\"\"Writer agent.\"\"\"\n",
    "    prompt = f\"Write content based on: {state['research']}\"\n",
    "    draft = llm.predict(prompt)\n",
    "    return {\"draft\": draft}\n",
    "\n",
    "def editor(state: MultiAgentState):\n",
    "    \"\"\"Editor agent.\"\"\"\n",
    "    prompt = f\"Edit and improve: {state['draft']}\"\n",
    "    final = llm.predict(prompt)\n",
    "    return {\"final\": final}\n",
    "\n",
    "# Build workflow\n",
    "workflow = StateGraph(MultiAgentState)\n",
    "workflow.add_node(\"researcher\", researcher)\n",
    "workflow.add_node(\"writer\", writer)\n",
    "workflow.add_node(\"editor\", editor)\n",
    "\n",
    "workflow.set_entry_point(\"researcher\")\n",
    "workflow.add_edge(\"researcher\", \"writer\")\n",
    "workflow.add_edge(\"writer\", \"editor\")\n",
    "workflow.add_edge(\"editor\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# Execute\n",
    "result = app.invoke({\n",
    "    \"task\": \"Future of AI\",\n",
    "    \"research\": \"\",\n",
    "    \"draft\": \"\",\n",
    "    \"final\": \"\"\n",
    "})\n",
    "\n",
    "print(f\"Final output: {result['final'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "âœ… **LangGraph Features:**\n",
    "- **State Management**: Shared state across nodes\n",
    "- **Conditional Routing**: Dynamic flow control\n",
    "- **Agent Loops**: Iterative reasoning\n",
    "- **Multi-Agent**: Specialized agent collaboration\n",
    "\n",
    "ðŸ“Œ **Use Cases:**\n",
    "- Complex decision workflows\n",
    "- Multi-step reasoning\n",
    "- Agent orchestration\n",
    "- Iterative refinement\n",
    "\n",
    "ðŸ”— **Resources:**\n",
    "- [LangGraph Docs](https://langchain-ai.github.io/langgraph/)\n",
    "- Follow [@BuildFastWithAI](https://twitter.com/BuildFastWithAI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
