{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
    "\n",
    "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
    "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/[NOTEBOOK_ID])\n",
    "\n",
    "## Master Generative AI in 8 Weeks\n",
    "**What You'll Learn:**\n",
    "- Master cutting-edge AI tools & frameworks\n",
    "- 6 weeks of hands-on, project-based learning\n",
    "- Weekly live mentorship sessions\n",
    "- No coding experience required\n",
    "- Join Innovation Community\n",
    "\n",
    "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
    "\n",
    "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini 3 Pro - Complete LangChain Guide\n",
    "\n",
    "**Created by:** @BuildFastWithAI  \n",
    "**Model:** Google Gemini 3 Pro  \n",
    "**Last Updated:** November 2025\n",
    "\n",
    "Comprehensive LangChain integration with Gemini 3 Pro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-google-genai faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple template\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-3-pro\", temperature=0.7)\n",
    "\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Write a short paragraph about {topic}.\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=template)\n",
    "result = chain.run(topic=\"artificial intelligence\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot template\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"hot\", \"output\": \"cold\"},\n",
    "]\n",
    "\n",
    "example_template = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\"\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_template,\n",
    "    prefix=\"Give the opposite of the word.\",\n",
    "    suffix=\"Input: {input}\\nOutput:\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt)\n",
    "print(chain.run(input=\"bright\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential chain\n",
    "synopsis_template = PromptTemplate(\n",
    "    input_variables=[\"title\"],\n",
    "    template=\"Write a synopsis for a movie titled '{title}'.\"\n",
    ")\n",
    "\n",
    "review_template = PromptTemplate(\n",
    "    input_variables=[\"synopsis\"],\n",
    "    template=\"Write a review for this movie synopsis:\\n{synopsis}\"\n",
    ")\n",
    "\n",
    "synopsis_chain = LLMChain(llm=llm, prompt=synopsis_template, output_key=\"synopsis\")\n",
    "review_chain = LLMChain(llm=llm, prompt=review_template, output_key=\"review\")\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[synopsis_chain, review_chain],\n",
    "    input_variables=[\"title\"],\n",
    "    output_variables=[\"synopsis\", \"review\"]\n",
    ")\n",
    "\n",
    "result = overall_chain({\"title\": \"AI Revolution\"})\n",
    "print(f\"Synopsis: {result['synopsis']}\\n\")\n",
    "print(f\"Review: {result['review']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation with memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation_template = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"],\n",
    "    template=\"\"\"\n",
    "Previous conversation:\n",
    "{history}\n",
    "\n",
    "Human: {input}\n",
    "AI:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "conversation_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=conversation_template,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "print(conversation_chain.run(input=\"Hi, I'm learning about AI.\"))\n",
    "print(\"\\n\" + conversation_chain.run(input=\"What did I just say?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Document Processing & RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain is a framework for developing LLM applications.\"),\n",
    "    Document(page_content=\"It provides tools for prompts, chains, and agents.\"),\n",
    "    Document(page_content=\"LangChain supports multiple LLM providers including Gemini.\")\n",
    "]\n",
    "\n",
    "# Create vector store\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "result = qa_chain({\"query\": \"What is LangChain?\"})\n",
    "print(f\"Answer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.tools import tool\n",
    "from langchain import hub\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "tools = [get_word_length, multiply]\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "result = agent_executor.invoke({\n",
    "    \"input\": \"How many letters in 'LangChain' times 5?\"\n",
    "})\n",
    "print(result[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LCEL (LangChain Expression Language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# LCEL chain\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} fact about {topic}\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    {\"adjective\": RunnablePassthrough(), \"topic\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"adjective\": \"interesting\", \"topic\": \"AI\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "âœ… **LangChain Components:**\n",
    "- **Prompts**: Templates and few-shot examples\n",
    "- **Chains**: Sequential and complex workflows\n",
    "- **Memory**: Conversation history\n",
    "- **Agents**: Autonomous decision-making\n",
    "- **LCEL**: Composable chains\n",
    "\n",
    "ðŸ“Œ **Best Practices:**\n",
    "- Use LCEL for modern chains\n",
    "- Leverage memory for conversations\n",
    "- Build modular components\n",
    "- Test chains thoroughly\n",
    "\n",
    "ðŸ”— **Resources:**\n",
    "- [LangChain Docs](https://python.langchain.com/)\n",
    "- Follow [@BuildFastWithAI](https://twitter.com/BuildFastWithAI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
