{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
    "\n",
    "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
    "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/[NOTEBOOK_ID])\n",
    "\n",
    "## Master Generative AI in 8 Weeks\n",
    "**What You'll Learn:**\n",
    "- Master cutting-edge AI tools & frameworks\n",
    "- 6 weeks of hands-on, project-based learning\n",
    "- Weekly live mentorship sessions\n",
    "- No coding experience required\n",
    "- Join Innovation Community\n",
    "\n",
    "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
    "\n",
    "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini 3 Pro - Specialized Use Cases\n",
    "\n",
    "**Created by:** @BuildFastWithAI  \n",
    "**Model:** Google Gemini 3 Pro  \n",
    "**Last Updated:** November 2025\n",
    "\n",
    "Advanced production use cases and integration patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-generativeai langchain langchain-google-genai fastapi pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.colab import userdata\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import json\n",
    "\n",
    "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Code Repository Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatGoogleGenerativeAI(model=\"gemini-3-pro\", temperature=0.3)\n",
    "    \n",
    "    def analyze_code(self, code: str, language: str = \"python\") -> dict:\n",
    "        \"\"\"Analyze code for quality and issues.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "Analyze this {language} code and provide:\n",
    "1. Code quality score (1-10)\n",
    "2. Potential bugs\n",
    "3. Performance issues\n",
    "4. Security concerns\n",
    "5. Suggestions for improvement\n",
    "\n",
    "Code:\n",
    "```{language}\n",
    "{code}\n",
    "```\n",
    "\n",
    "Return as JSON.\n",
    "\"\"\"\n",
    "        \n",
    "        response = self.llm.predict(prompt)\n",
    "        try:\n",
    "            return json.loads(response)\n",
    "        except:\n",
    "            return {\"analysis\": response}\n",
    "    \n",
    "    def generate_tests(self, code: str, language: str = \"python\") -> str:\n",
    "        \"\"\"Generate unit tests for code.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "Generate comprehensive unit tests for this {language} code:\n",
    "\n",
    "```{language}\n",
    "{code}\n",
    "```\n",
    "\n",
    "Include:\n",
    "- Normal cases\n",
    "- Edge cases\n",
    "- Error cases\n",
    "\"\"\"\n",
    "        \n",
    "        return self.llm.predict(prompt)\n",
    "    \n",
    "    def document_code(self, code: str, language: str = \"python\") -> str:\n",
    "        \"\"\"Generate documentation for code.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "Generate detailed documentation for this {language} code:\n",
    "\n",
    "```{language}\n",
    "{code}\n",
    "```\n",
    "\n",
    "Include:\n",
    "- Function descriptions\n",
    "- Parameter explanations\n",
    "- Return value descriptions\n",
    "- Usage examples\n",
    "\"\"\"\n",
    "        \n",
    "        return self.llm.predict(prompt)\n",
    "\n",
    "# Test\n",
    "analyzer = CodeAnalyzer()\n",
    "\n",
    "sample_code = \"\"\"\n",
    "def calculate_average(numbers):\n",
    "    total = sum(numbers)\n",
    "    return total / len(numbers)\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Code Analysis ===\")\n",
    "analysis = analyzer.analyze_code(sample_code)\n",
    "print(json.dumps(analysis, indent=2))\n",
    "\n",
    "print(\"\\n=== Generated Tests ===\")\n",
    "tests = analyzer.generate_tests(sample_code)\n",
    "print(tests[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Production API Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "import uvicorn\n",
    "\n",
    "# Define models\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str\n",
    "    temperature: Optional[float] = 0.7\n",
    "    max_tokens: Optional[int] = 1024\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    response: str\n",
    "    model: str\n",
    "    tokens_used: int\n",
    "\n",
    "# Create API\n",
    "app = FastAPI(title=\"Gemini 3 Pro API\")\n",
    "\n",
    "@app.post(\"/chat\", response_model=ChatResponse)\n",
    "async def chat(request: ChatRequest):\n",
    "    \"\"\"Chat endpoint.\"\"\"\n",
    "    try:\n",
    "        model = genai.GenerativeModel(\n",
    "            'gemini-3-pro',\n",
    "            generation_config={\n",
    "                \"temperature\": request.temperature,\n",
    "                \"max_output_tokens\": request.max_tokens\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        response = model.generate_content(request.message)\n",
    "        \n",
    "        return ChatResponse(\n",
    "            response=response.text,\n",
    "            model=\"gemini-3-pro\",\n",
    "            tokens_used=len(request.message.split()) + len(response.text.split())\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"healthy\", \"model\": \"gemini-3-pro\"}\n",
    "\n",
    "# To run: uvicorn main:app --reload\n",
    "print(\"API defined. In production, run with: uvicorn main:app --host 0.0.0.0 --port 8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Evaluation & Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List, Dict\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self):\n",
    "        self.model = genai.GenerativeModel('gemini-3-pro')\n",
    "        self.llm = ChatGoogleGenerativeAI(model=\"gemini-3-pro\")\n",
    "    \n",
    "    def evaluate_accuracy(self, qa_pairs: List[Dict]) -> dict:\n",
    "        \"\"\"Evaluate model accuracy on Q&A pairs.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for pair in qa_pairs:\n",
    "            question = pair[\"question\"]\n",
    "            expected = pair[\"expected_answer\"]\n",
    "            \n",
    "            response = self.model.generate_content(question)\n",
    "            actual = response.text\n",
    "            \n",
    "            # Evaluate similarity\n",
    "            eval_prompt = f\"\"\"\n",
    "Rate how similar these answers are (0-10):\n",
    "\n",
    "Expected: {expected}\n",
    "Actual: {actual}\n",
    "\n",
    "Return only the number.\n",
    "\"\"\"\n",
    "            \n",
    "            score = float(self.llm.predict(eval_prompt).strip())\n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"score\": score,\n",
    "                \"actual\": actual[:100]\n",
    "            })\n",
    "        \n",
    "        avg_score = sum(r[\"score\"] for r in results) / len(results)\n",
    "        \n",
    "        return {\n",
    "            \"average_score\": avg_score,\n",
    "            \"results\": results\n",
    "        }\n",
    "    \n",
    "    def benchmark_performance(self, prompts: List[str]) -> dict:\n",
    "        \"\"\"Benchmark model performance.\"\"\"\n",
    "        times = []\n",
    "        token_counts = []\n",
    "        \n",
    "        for prompt in prompts:\n",
    "            start = time.time()\n",
    "            response = self.model.generate_content(prompt)\n",
    "            elapsed = time.time() - start\n",
    "            \n",
    "            times.append(elapsed)\n",
    "            token_counts.append(len(prompt.split()) + len(response.text.split()))\n",
    "        \n",
    "        return {\n",
    "            \"avg_latency\": sum(times) / len(times),\n",
    "            \"min_latency\": min(times),\n",
    "            \"max_latency\": max(times),\n",
    "            \"avg_tokens\": sum(token_counts) / len(token_counts),\n",
    "            \"throughput\": len(prompts) / sum(times)\n",
    "        }\n",
    "\n",
    "# Test evaluation\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "qa_pairs = [\n",
    "    {\n",
    "        \"question\": \"What is 2+2?\",\n",
    "        \"expected_answer\": \"4\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the capital of France?\",\n",
    "        \"expected_answer\": \"Paris\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=== Accuracy Evaluation ===\")\n",
    "accuracy = evaluator.evaluate_accuracy(qa_pairs)\n",
    "print(f\"Average Score: {accuracy['average_score']:.2f}/10\")\n",
    "\n",
    "print(\"\\n=== Performance Benchmark ===\")\n",
    "benchmark = evaluator.benchmark_performance([\n",
    "    \"Explain AI\",\n",
    "    \"What is ML?\",\n",
    "    \"Define quantum computing\"\n",
    "])\n",
    "print(json.dumps(benchmark, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Monitoring & Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "class ModelMonitor:\n",
    "    def __init__(self):\n",
    "        self.model = genai.GenerativeModel('gemini-3-pro')\n",
    "        self.metrics = defaultdict(list)\n",
    "    \n",
    "    def tracked_generate(self, prompt: str, **kwargs) -> dict:\n",
    "        \"\"\"Generate content with tracking.\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = self.model.generate_content(prompt, **kwargs)\n",
    "            elapsed = time.time() - start\n",
    "            \n",
    "            metrics = {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"prompt_length\": len(prompt),\n",
    "                \"response_length\": len(response.text),\n",
    "                \"latency\": elapsed,\n",
    "                \"status\": \"success\",\n",
    "                \"error\": None\n",
    "            }\n",
    "            \n",
    "            result = {\"text\": response.text, \"metrics\": metrics}\n",
    "            \n",
    "        except Exception as e:\n",
    "            elapsed = time.time() - start\n",
    "            metrics = {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"prompt_length\": len(prompt),\n",
    "                \"latency\": elapsed,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            result = {\"text\": None, \"metrics\": metrics}\n",
    "        \n",
    "        # Store metrics\n",
    "        for key, value in metrics.items():\n",
    "            self.metrics[key].append(value)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Get aggregated statistics.\"\"\"\n",
    "        latencies = [m for m in self.metrics[\"latency\"] if isinstance(m, (int, float))]\n",
    "        \n",
    "        return {\n",
    "            \"total_requests\": len(self.metrics[\"timestamp\"]),\n",
    "            \"success_rate\": sum(1 for s in self.metrics[\"status\"] if s == \"success\") / len(self.metrics[\"status\"]),\n",
    "            \"avg_latency\": sum(latencies) / len(latencies) if latencies else 0,\n",
    "            \"p95_latency\": sorted(latencies)[int(len(latencies) * 0.95)] if latencies else 0,\n",
    "            \"avg_prompt_length\": sum(self.metrics[\"prompt_length\"]) / len(self.metrics[\"prompt_length\"]),\n",
    "            \"errors\": [e for e in self.metrics[\"error\"] if e is not None]\n",
    "        }\n",
    "\n",
    "# Test monitoring\n",
    "monitor = ModelMonitor()\n",
    "\n",
    "# Make some requests\n",
    "for prompt in [\"What is AI?\", \"Explain ML\", \"Define data science\"]:\n",
    "    result = monitor.tracked_generate(prompt)\n",
    "    print(f\"Response: {result['text'][:100]}...\")\n",
    "\n",
    "# Get stats\n",
    "print(\"\\n=== Monitoring Stats ===\")\n",
    "stats = monitor.get_stats()\n",
    "print(json.dumps(stats, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cost Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostOptimizer:\n",
    "    def __init__(self, cost_per_1k_tokens: float = 0.001):\n",
    "        self.model = genai.GenerativeModel('gemini-3-pro')\n",
    "        self.cost_per_1k = cost_per_1k_tokens\n",
    "        self.cache = {}\n",
    "    \n",
    "    def estimate_cost(self, prompt: str, response: str = None) -> float:\n",
    "        \"\"\"Estimate request cost.\"\"\"\n",
    "        prompt_tokens = len(prompt.split())\n",
    "        response_tokens = len(response.split()) if response else 100  # Estimate\n",
    "        total_tokens = prompt_tokens + response_tokens\n",
    "        return (total_tokens / 1000) * self.cost_per_1k\n",
    "    \n",
    "    def optimize_prompt(self, prompt: str) -> str:\n",
    "        \"\"\"Optimize prompt for cost.\"\"\"\n",
    "        # Remove unnecessary whitespace\n",
    "        optimized = \" \".join(prompt.split())\n",
    "        \n",
    "        # Estimate savings\n",
    "        original_cost = self.estimate_cost(prompt)\n",
    "        optimized_cost = self.estimate_cost(optimized)\n",
    "        savings = original_cost - optimized_cost\n",
    "        \n",
    "        print(f\"Original cost: ${original_cost:.6f}\")\n",
    "        print(f\"Optimized cost: ${optimized_cost:.6f}\")\n",
    "        print(f\"Savings: ${savings:.6f}\")\n",
    "        \n",
    "        return optimized\n",
    "    \n",
    "    def cached_generate(self, prompt: str) -> dict:\n",
    "        \"\"\"Generate with caching to reduce costs.\"\"\"\n",
    "        if prompt in self.cache:\n",
    "            return {\n",
    "                \"text\": self.cache[prompt],\n",
    "                \"cached\": True,\n",
    "                \"cost\": 0\n",
    "            }\n",
    "        \n",
    "        response = self.model.generate_content(prompt)\n",
    "        self.cache[prompt] = response.text\n",
    "        \n",
    "        return {\n",
    "            \"text\": response.text,\n",
    "            \"cached\": False,\n",
    "            \"cost\": self.estimate_cost(prompt, response.text)\n",
    "        }\n",
    "\n",
    "# Test optimization\n",
    "optimizer = CostOptimizer()\n",
    "\n",
    "prompt = \"\"\"What    is    artificial    intelligence?    \n",
    "Please    explain    in    detail.\"\"\"\n",
    "\n",
    "print(\"=== Prompt Optimization ===\")\n",
    "optimized = optimizer.optimize_prompt(prompt)\n",
    "\n",
    "print(\"\\n=== Caching Benefit ===\")\n",
    "result1 = optimizer.cached_generate(\"What is AI?\")\n",
    "print(f\"First call - Cost: ${result1['cost']:.6f}\")\n",
    "\n",
    "result2 = optimizer.cached_generate(\"What is AI?\")\n",
    "print(f\"Second call (cached) - Cost: ${result2['cost']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "âœ… **Production Patterns:**\n",
    "- Code analysis and generation\n",
    "- API integration with FastAPI\n",
    "- Comprehensive evaluation\n",
    "- Real-time monitoring\n",
    "- Cost optimization\n",
    "\n",
    "ðŸ“Œ **Best Practices:**\n",
    "- Implement caching\n",
    "- Monitor performance metrics\n",
    "- Optimize prompts for cost\n",
    "- Add comprehensive error handling\n",
    "- Use structured logging\n",
    "\n",
    "ðŸš€ **Deployment Checklist:**\n",
    "- [ ] API rate limiting\n",
    "- [ ] Cost monitoring\n",
    "- [ ] Error tracking\n",
    "- [ ] Performance metrics\n",
    "- [ ] Security measures\n",
    "- [ ] Caching strategy\n",
    "- [ ] Load testing\n",
    "\n",
    "ðŸ”— **Resources:**\n",
    "- [Production Best Practices](https://ai.google.dev/)\n",
    "- Follow [@BuildFastWithAI](https://twitter.com/BuildFastWithAI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
