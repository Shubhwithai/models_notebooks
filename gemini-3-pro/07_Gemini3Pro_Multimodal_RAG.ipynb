{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1wYSMgJtARFdvTt5g7E20mE4NmwUFUuog\" width=\"200\">\n",
    "\n",
    "[![Gen AI Experiments](https://img.shields.io/badge/Gen%20AI%20Experiments-GenAI%20Bootcamp-blue?style=for-the-badge&logo=artificial-intelligence)](https://github.com/buildfastwithai/gen-ai-experiments)\n",
    "[![Gen AI Experiments GitHub](https://img.shields.io/github/stars/buildfastwithai/gen-ai-experiments?style=for-the-badge&logo=github&color=gold)](http://github.com/buildfastwithai/gen-ai-experiments)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/[NOTEBOOK_ID])\n",
    "\n",
    "## Master Generative AI in 8 Weeks\n",
    "**What You'll Learn:**\n",
    "- Master cutting-edge AI tools & frameworks\n",
    "- 6 weeks of hands-on, project-based learning\n",
    "- Weekly live mentorship sessions\n",
    "- No coding experience required\n",
    "- Join Innovation Community\n",
    "\n",
    "Transform your AI ideas into reality through hands-on projects and expert mentorship.\n",
    "\n",
    "[Start Your Journey](https://www.buildfastwithai.com/genai-course)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini 3 Pro - Multimodal RAG\n",
    "\n",
    "**Created by:** @BuildFastWithAI  \n",
    "**Model:** Google Gemini 3 Pro  \n",
    "**Last Updated:** November 2025\n",
    "\n",
    "Build RAG systems that handle text, images, and documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-generativeai pillow pypdf2 pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.colab import userdata\n",
    "from PIL import Image\n",
    "import io\n",
    "import base64\n",
    "\n",
    "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample image\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "img = Image.new('RGB', (400, 200), color='white')\n",
    "draw = ImageDraw.Draw(img)\n",
    "draw.text((50, 80), \"Sample Chart Data\", fill='black')\n",
    "img.save('sample_chart.png')\n",
    "\n",
    "# Analyze image\n",
    "model = genai.GenerativeModel('gemini-3-pro-vision')\n",
    "\n",
    "img = Image.open('sample_chart.png')\n",
    "response = model.generate_content([\n",
    "    \"Describe this image in detail.\",\n",
    "    img\n",
    "])\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visual Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQA with Gemini\n",
    "def visual_qa(image_path: str, question: str) -> str:\n",
    "    \"\"\"Answer questions about an image.\"\"\"\n",
    "    model = genai.GenerativeModel('gemini-3-pro-vision')\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    response = model.generate_content([question, img])\n",
    "    return response.text\n",
    "\n",
    "# Test VQA\n",
    "answer = visual_qa(\n",
    "    'sample_chart.png',\n",
    "    'What text is visible in this image?'\n",
    ")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Processing with Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDocument:\n",
    "    def __init__(self, text: str = \"\", images: list = None):\n",
    "        self.text = text\n",
    "        self.images = images or []\n",
    "        self.image_descriptions = []\n",
    "    \n",
    "    def process_images(self, model):\n",
    "        \"\"\"Generate descriptions for all images.\"\"\"\n",
    "        for img_path in self.images:\n",
    "            img = Image.open(img_path)\n",
    "            response = model.generate_content([\n",
    "                \"Describe this image concisely.\",\n",
    "                img\n",
    "            ])\n",
    "            self.image_descriptions.append(response.text)\n",
    "    \n",
    "    def get_full_content(self) -> str:\n",
    "        \"\"\"Get combined text and image descriptions.\"\"\"\n",
    "        content = self.text\n",
    "        for i, desc in enumerate(self.image_descriptions, 1):\n",
    "            content += f\"\\n\\nImage {i}: {desc}\"\n",
    "        return content\n",
    "\n",
    "# Test\n",
    "doc = MultimodalDocument(\n",
    "    text=\"This document contains analysis.\",\n",
    "    images=['sample_chart.png']\n",
    ")\n",
    "\n",
    "vision_model = genai.GenerativeModel('gemini-3-pro-vision')\n",
    "doc.process_images(vision_model)\n",
    "\n",
    "print(doc.get_full_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multimodal RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "class MultimodalRAG:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/embedding-001\",\n",
    "            google_api_key=api_key\n",
    "        )\n",
    "        self.text_llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-3-pro\",\n",
    "            google_api_key=api_key\n",
    "        )\n",
    "        self.vision_model = genai.GenerativeModel('gemini-3-pro-vision')\n",
    "        self.vectorstore = None\n",
    "        self.documents = []\n",
    "    \n",
    "    def add_document(self, text: str, images: list = None):\n",
    "        \"\"\"Add multimodal document.\"\"\"\n",
    "        doc = MultimodalDocument(text, images)\n",
    "        if images:\n",
    "            doc.process_images(self.vision_model)\n",
    "        \n",
    "        self.documents.append(doc)\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"Build vector index from documents.\"\"\"\n",
    "        docs = [\n",
    "            Document(page_content=doc.get_full_content())\n",
    "            for doc in self.documents\n",
    "        ]\n",
    "        self.vectorstore = FAISS.from_documents(docs, self.embeddings)\n",
    "    \n",
    "    def query(self, question: str, k: int = 3) -> str:\n",
    "        \"\"\"Query the multimodal RAG.\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            return \"No index built\"\n",
    "        \n",
    "        # Retrieve\n",
    "        docs = self.vectorstore.similarity_search(question, k=k)\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        # Generate\n",
    "        prompt = f\"\"\"\n",
    "Answer based on the context (includes text and image descriptions).\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "        \n",
    "        return self.text_llm.predict(prompt)\n",
    "\n",
    "# Test multimodal RAG\n",
    "rag = MultimodalRAG(GOOGLE_API_KEY)\n",
    "\n",
    "rag.add_document(\n",
    "    text=\"Q4 sales report shows growth.\",\n",
    "    images=['sample_chart.png']\n",
    ")\n",
    "rag.add_document(\n",
    "    text=\"Revenue increased by 25% year over year.\"\n",
    ")\n",
    "\n",
    "rag.build_index()\n",
    "\n",
    "answer = rag.query(\"What does the report show?\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "âœ… **Multimodal Capabilities:**\n",
    "- Image understanding and captioning\n",
    "- Visual question answering\n",
    "- Document processing with images\n",
    "- Cross-modal retrieval\n",
    "\n",
    "ðŸ“Œ **Use Cases:**\n",
    "- Document Q&A with charts/diagrams\n",
    "- Product catalogs with images\n",
    "- Research papers with figures\n",
    "- Medical records with scans\n",
    "\n",
    "ðŸ”— **Resources:**\n",
    "- Follow [@BuildFastWithAI](https://twitter.com/BuildFastWithAI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
